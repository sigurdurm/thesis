% Chapter 3

\chapter{Related Work} % Write in your own chapter title
\label{Chapter3}
\lhead{Chapter 3. \emph{Related Work}} % Write in your own chapter title to set the page header

Clustering player behaviors and processing large data sets have been researched actively in the recent years. In our work we find the general player behaviors in a the case study using k-means clustering algorithm in the MapReduce framework for high scalability and parallel processing of large-scale data. The behavior of the data can change from day to day like when dealing with an endless stream of data is also discussed. In subsequent sections some of the recent and related work are given a short introduction.

\section{Clustering Player Behaviors}
Many researches have been done on clustering and predicting player behaviors over the last years to get a better understanding which kind of user behaviors are to be found when playing a game that can be actionable for game developers \citep{Marsh:2006Continuous, Missura2009Player, Thurau:2009SVIM, Drachen:2011Evaluating, Drachen:2012}. User behavior analysis has becoming increasingly popular in the recent years because of rise of the free-to-play (F2P) genre games in \textit{Facebook} and \textit{Google Play} where populations can be in millions creating complex game interactions \citep{Kim:2008Tracking, Drachen:2011Evaluating}. Playing these games are free and many are of persistent nature where the world in the game continues when a player exits. To be profitable these games drive their revenue via micro transactions, e.g. players buying upgrades or virtual items in game for real money \citep{Kim:2008Tracking, Drachen:2011Evaluating, Fields:2011SocialGame, Seif:2013GameAnalytics}. Major game publishers have also been collecting and analyzing large scale of behavior telemetry data but details of their methods are kept confidential \citep{Zoeller:2010, Yannakakis:2012}. Most available research work is case-based where a specific algorithm is applied to a specific game and commercial game data sets has only become accessible recently for academic researchers \citep{Yannakakis:2012}.

Predicting player behavior in a major commercial game Tomb Raider: Underworld (TRU) was presented in a study of Drachen et al. \citep{Drachen:2009Tomb}. Authors classified 1365 players in a moderate data set into four user behavioral groups using six statistical gameplay features based on core game design as inputs of an emergent self-organizing map to identify dissimilar behavior clusters. Behavior profiles covering 90 percent of the users in the dataset were labeled in game terminology usable for game designers. Mahlmann et al. \citep{Mahlmann:2010Tomb} did a follow up on the research using eight gameplay features and classified behavior of 10,000 players. The authors presented also how to predict behavior based on early play analysis, a popular topic which can be used to prevent churn (attrition) \citep{Fields:2011SocialGame}.

Analyzing social groups in the highly popular Massively Multiplayer Online Role-Playing Game (MMORPG) World of Warcraft was done by Thurau and Bauckhage \citep{Thurau:2010WoW}. They analyzed how groups (guilds) evolve over time from both American and European based guilds. Their paper is the first study analyzing such amount of data in a MMORPG, analyzing large-scale data gathered on-line from 18 million players belonging in 1.4 million groups over a period of 4 years. Convex-Hull Non Negative Matrix Factorization (CH-NMF) \citep{Thurau:2009SVIM} technique was applied to the data to find the extremes rather than averages and the results show  no significant cultural difference in formation processes of guilds from either the US or the EU. Interpretability of CH-NMF was more distinguishable and representing archetypal guilds than the more conventional clustering method k-means that represent the cluster centroids with similar characteristic.

Drachen et al. \citep{Drachen:2012} did a clustering analysis for two major commercial games applied to large-scale of high-dimensionality player behavior telemetry. K-means and Simplex Volume Maximization (SIVM) clustering were applied to the MMORPG \textit{Tera} and the multi-player first-person shooter strategy game \textit{Battlefield: Bad Company 2}. SIVM clustering is an adaption of Archetype Analysis (AA) for large-scale data sets to find extreme player behaviors profiles \citep{Thurau:2009SVIM, Kersting:2010SVIM}. The authors show the contribution differences from the two algorithms where k-means gives insights into the general distribution of behaviors vs. SIVM showing players with extreme behaviors. The selection of the most important features from the data set were followed by a method suggested by Drachen et al. \citep{Drachen:2009Tomb}, behavioral profiles were extracted and interpreted in terms of design language \citep{Drachen:2009Tomb, Zoeller:2010}.

In a recent study by Drachen et al. \citep{Drachen:2013} the authors compare four different popular methods with purpose of clustering player behaviors and develop profiles from large-scale game metric data set from the highly popular commercial MMORPG World of Warcraft. The data set was collected from mining the Warcraft Realms site, recordings of on-line time and what level each player reached for each day in the years 2005-2010 for approx. 70 thousands of players. The authors selected playtime and leveling speed as their behavioral variables to show a measure of the overall player engagement in the game, where playtime is one of the most important measure for calculating the churn rate \citep{Fields:2011SocialGame, Seif:2013GameAnalytics}. Interpretable behaviors profiles where only generated by the k-means and the SIVM algorithm. The SIVM Archetype Analysis algorithm produces however significantly different behaviors that result in easier interpretation of behavior profiles compared to the k-means algorithm where the centroids are overall similar.	

\section{Clustering Large Data}
K-means \citep{FORGYE.W.:1965, MacQueen:1967KMeans, Lloyd:1982} is one of the most studied clustering algorithm out there and is still actively researched. It's a simple algorithm that partition the data into \textit{k} partitions by minimizing its objective function sum of squared error (SSQ). From its appearance it has been one of the most popular clustering algorithm to research because its ease of interpretation and simplicity \citep{Xu:2005Clustering, Rokach:2010Survey}. One of the problems with k-means it is a heuristic algorithm and has no guarantee to converge to a global optimum (optimal solution). Many work have been done in researching approximation guarantees (guarantee a approximated solution which is a within a constant-factor of the optimal solution) for k-means both in non-streaming and streaming versions \citep{Kanungo:2002KM, Arthur:2007, Ailon:2009, Braverman:2011, Shindler:2011}. In recent years k-means has also been very popular algorithm to study in the MapReduce framework where the algorithm can easily be applied to cluster large amount of data sets in parallel \citep{Dean:2004, Zhao:2009, Ngazimbi:2009MSc, Christopoulos:2011Thesis, Ramamoorthy:2011MSc}.

Guha et al. \citep{Guha:2003} designed an algorithm in 2003 called STREAM that is based on the divide and conquer strategy to solve the k-median problem (a k-means variant), authors guarantee a approximated guaranteed solution. The algorithm divides the dataset into \textit{m} pieces of similar sizes, where each of the pieces are independently clustered sequentially and all the centers from all the pieces are then clustered further. They show a new k-median algorithm called LSEARCH that is used by the STREAM algorithm and is based on a local search algorithm solving the facility location problem \citep{Charikar:1999} to solve the k-median problem. Results show that STREAM LSEARCH produced near optimal quality clusters and better than STREAM k-means also with smaller variance. Compared to the the hierarchical algorithm BIRCH \citep{Zhang:1996} it took 2-3 times longer to run but produced 2-3 times better quality clusters (SSQ), showing superior strength when the goal is to minimize the SSQ like detecting intrusions in networks \citep{Marchette:1999NI}. 

In 2009 Ailon et al. \citep{Ailon:2009} extended the the work of Guha et al. mentioned above by introducing a new single pass streaming algorithm for k-means, first of its kind with approximation guarantees. Achieving this they also designed a new algorithm called k-means\# that is based on a randomized seeding procedure from the non-streaming algorithm k-means++ by Arthur and Vassilvitskii \citep{Arthur:2007}. The k-means++ chooses \textit{k} centers non-uniformly whereas k-means\# selects $O(k\log{k})$ centers and achieves a constant approximation guarantee. In the streaming algorithm they run the k-means\# independently on each divided piece of the data to achieve $O(k \log{k})$ random centers non-uniformly and use the k-means++ algorithm to find \textit{k} centers from the intermediate centers from all the pieces of the data set. 

Another approach using a \textit{coreset} by selecting a weighted subset from the original dataset such that by running any k-means algorithm on the subset will give near similar results to running k-means on the whole dataset. Ackermann et al. \citep{Ackermann:2010} introduced a streaming algorithm called StreamKM++ that is a streaming version of k-means++ algorithm from Arthur et al. \citep{Arthur:2007} to solve k-means on the weighted subset and a new data structure called coreset tree, speeding up the time for the sampling in the center initialization. Their approach was shown to produce similar quality of clusters (in terms of SSQ) as the STREAM LSEARCH \citep{Guha:2003} algorithm but scaling much better with number of clusters centers. 

Shindler et al. \citep{Shindler:2011} proposed an algorithm called \textit{Fast streaming k-means} based on the online facility location algorithm \citep{Meyerson:2001} and extends the work of Braverman et al. \citep{Braverman:2011}. Authors prove that their algorithm has a much faster running time and often better cluster quality than the divide and conquer algorithm introduced by Ailon et al. \citep{Ailon:2009}. The algorithm however show similar average results as StreamKM++ \citep{Ackermann:2010} in both running time and quality tho with better accuracy.

Clustering data streams of an unknown length, evolving over time \citep{Aggarwal:2002, Aggarwal:2003Evolving} are challenging where it is not possible to access historic data points because of the amount of data arriving continuously. Aggarwal et al. \citep{Aggarwal:2003} proposed a well-known stream clustering framework called CluStream for clustering large evolving data streams and is guided by application-centered requirements. CluStream has an online component that maintains snapshots of statistical information about micro-clusters (a.k.a. \textit{cluster feature vector} \citep{Zhang:1996}) in a pyramidal time window and an offline component that uses the compact intermediate summary statistics from the micro-clusters to find higher level \textit{k} clusters using k-means, in a time horizon defined by an analyst. A new high-dimensional highly scalable data stream clustering algorithm called HPStream was also proposed \citep{Aggarwal:2004}. HPStream uses projected clustering \citep{Aggarwal:1999}, which can determine clusters for a subset of dimensions, to data streams and a new data structure called \textit{fading cluster structure} that allows historical and current data to integrate nicely with a user-specified fading factor. 

Another approach clustering evolving streams Zhou et al. \citep{Zhou:2008} presented a algorithm called SWClustering to cluster evolving data streams over so called sliding windows, where the most recent records are considered to be more critical than historic data \citep{Datar:2002SW}. Allowing to analyze the evolution of the individual clusters by eliminating influence by outdated historic data points when new data points arrive. Authors show that the CluStream algorithm \citep{Aggarwal:2003} is much more sensitive to influences of outdated data and is less efficient.

Processing large of amount of data efficiently using parallel processing is an active research and gain much of popularity when the Google's MapReduce programming model was introduced by Dean and Ghemawat \citep{Dean:2004}. Zhao et al. \citep{Zhao:2009} implemented a parallel version of k-means (PKMeans) in the MapReduce framework and showing their algorithm can be effectively run on large data sets. The Map function calculates the distance to the closest cluster centroid for each data point at a time, after assigning to the clusters a combiner sums up all the data points dimensions for each cluster from that Map function and outputs the key and value $<$\textit{cluster centroid, [sum for all dimensions, number of points]} $>$. The Reduce function then sums up all the intermediate sum values for each cluster and calculates the mean for the new centroids. 

Li et al. \citep{Li:2011} implemented the algorithm MBK-means in MapReduce using the Bagging ensemble learning method \citep{Breiman:1996}, using replacement sampling to generate  \textit{k} new data sets from the original data. K-means algorithm clusters each new data set using the MapReduce framework until convergence and in the end all the centroids from the k sets are merged to form the final k centroids.

Many extensions on the traditional MapReduce framework have been proposed to support efficient algorithms running iteratively \citep{Condie:2010HadoopOnline, Ekanayake:2010Twister, Zaharia:2010Spark, Bu:2010HaLoop, Bu:2012HaLoop, Yan:2012IncMr} and incrementally \citep{Bhatotia:2011Incoop, Yan:2012IncMr, Bhatotia:2012Slider}. The incremental MapReduce frameworks are interesting and relates to our work since we are incrementally clustering chunks of data but are not under study in this thesis.

\section{Our study}
The algorithm in this thesis is most similar to PKMeans \citep{Zhao:2009} described above, a parallel k-means implementation in MapReduce using a Combine function to reduce the intermediate data sent between the mappers and the reducers. We however implement a parallel k-means algorithm in MapReduce so that each Map function efficiently calculates the distance to nearest cluster centers by calculating the distance matrix between all data points and the cluster centers instead of processing each data point separately. We apply the algorithm to incrementally but non-iteratively cluster player behaviors on theoretically large data sets that arrive daily. 

In our work we show k-means is a good approach to provide valuable insights into the general of behaviors for a specific real game data provided by GameAnalytics \citep{GA2013}. Work of Drachen et al. \citep{Drachen:2012, Drachen:2013} relates to ours when selecting and building important behavioral variables from user's telemetry and extracting behavioral profiles by analyzing and interpret the centroids (basis vectors) from the k-means algorithm running in MapReduce. Additionally we perform experiments with a controlled data set where we have different normal distributions of data arriving separately each day with results and future work are discussed.