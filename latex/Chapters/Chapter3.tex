% Chapter 3

\chapter{Related Work} % Write in your own chapter title
\label{Chapter3}
\lhead{Chapter 3. \emph{Related Work}} % Write in your own chapter title to set the page header

Clustering player behaviors and processing large data sets have been researched actively in the recent years. In our work we find the general player behaviors in a the case study using k-means clustering algorithm in the MapReduce framework for high scalability and parallel processing of large-scale data. The behavior of the data can change from day to day like when dealing with an endless stream of data is also discussed. In subsequent sections some of the recent and related work are given a short introduction.

\section{Clustering Player Behaviors}
Many researches have been done on clustering and predicting player behaviors over the last years to get a better understanding which kind of user behaviors are to be found when playing a game that can be actionable for game developers \citep{Marsh:2006Continuous, Missura2009Player, Thurau:2009SVIM, Drachen:2011Evaluating, Drachen:2012}. User behavior analysis has becoming increasingly popular in the recent years because of rise of the free-to-play (F2P) genre games in \textit{Facebook} and \textit{Google Play} where populations can be in millions creating complex game interactions \citep{Kim:2008Tracking, Drachen:2011Evaluating}. Playing these games are free and many are of persistent nature where the world in the game continues when a player exits. To be profitable these games drive their revenue via micro transactions, e.g. players buying upgrades or virtual items in game for real money \citep{Kim:2008Tracking, Drachen:2011Evaluating, Fields:2011SocialGame, Seif:2013GameAnalytics}. Major game publishers have also been collecting and analyzing large scale of behavior telemetry data but details of their methods are kept confidential \citep{Zoeller:2010, Yannakakis:2012}. Most available research work is case-based where a specific algorithm is applied to a specific game and commercial game data sets has only become accessible recently for academic researchers \citep{Yannakakis:2012}.

Predicting player behavior in a major commercial game Tomb Raider: Underworld (TRU) was presented in a study of Drachen et al. \citep{Drachen:2009Tomb}. Authors classified 1365 players in a moderate data set into four user behavioral groups using six statistical gameplay features based on core game design as inputs of an emergent self-organizing map to identify dissimilar behavior clusters. Behavior profiles covering 90 percent of the users in the dataset were labeled in game terminology usable for game designers. Mahlmann et al. \citep{Mahlmann:2010Tomb} did a follow up on the research using eight gameplay features and classified behavior of 10,000 players. The authors presented also how to predict behavior based on early play analysis, a popular topic which can be used to prevent churn (attrition) \citep{Fields:2011SocialGame}.

Analyzing social groups in the highly popular Massively Multiplayer Online Role-Playing Game (MMORPG) World of Warcraft was done by Thurau and Bauckhage \citep{Thurau:2010WoW}. They analyzed how groups (guilds) evolve over time from both American and European based guilds. Their paper is the first study analyzing such amount of data in a MMORPG, analyzing large-scale data gathered on-line from 18 million players belonging in 1.4 million groups over a period of 4 years. Convex-Hull Non Negative Matrix Factorization (CH-NMF) \citep{Thurau:2009SVIM} technique was applied to the data to find the extremes rather than averages and the results show  no significant cultural difference in formation processes of guilds from either the US or the EU. Interpretability of CH-NMF was more distinguishable and representing archetypal guilds than the more conventional clustering method k-means that represent the cluster centroids with similar characteristic.

Drachen et al. \citep{Drachen:2012} did a clustering analysis for two major commercial games applied to large-scale of high-dimensionality player behavior telemetry. K-means and Simplex Volume Maximization (SIVM) clustering were applied to the MMORPG \textit{Tera} and the multi-player first-person shooter strategy game \textit{Battlefield: Bad Company 2}. SIVM clustering is an adaption of Archetype Analysis (AA) for large-scale data sets to find extreme player behaviors profiles \citep{Thurau:2009SVIM, Kersting:2010SVIM}. The authors show the contribution differences from the two algorithms where k-means gives insights into the general distribution of behaviors vs. SIVM showing players with extreme behaviors. The selection of the most important features from the data set were followed by a method suggested by Drachen et al. \citep{Drachen:2009Tomb}, behavioral profiles were extracted and interpreted in terms of design language \citep{Drachen:2009Tomb, Zoeller:2010}.

In a recent study by Drachen et al. \citep{Drachen:2013} the authors compare four different popular methods with purpose of clustering player behaviors and develop profiles from large-scale game metric data set from the highly popular commercial MMORPG World of Warcraft. The data set was collected from mining the Warcraft Realms site, recordings of on-line time and what level each player reached for each day in the years 2005-2010 for approx. 70 thousands of players. The authors selected playtime and leveling speed as their behavioral variables to show a measure of the overall player engagement in the game, where playtime is one of the most important measure for calculating the churn rate \citep{Fields:2011SocialGame, Seif:2013GameAnalytics}. Interpretable behaviors profiles where only generated by the k-means and the SIVM algorithm. The SIVM Archetype Analysis algorithm produces however significantly different behaviors that result in easier interpretation of behavior profiles compared to the k-means algorithm where the centroids are overall similar.	

\section{Clustering Large Data}
K-means is one of the most studied clustering algorithm out there and is still actively researched. It's a simple algorithm that partition the data into \textit{k} partitions by minimizing its objective function sum of squared error. From its appearance in a standard algorithm version by Stuart Lloyd in 1982 \citep{Lloyd:1982}, it has been one of the most popular clustering algorithm to research because of its simplicity. There are many different research areas regarding k-means e.g. manually set the number of \textit{k} partitions to cluster, initializing the centers of the partitions, dealing with outliers in data and scalability when dataset increases. In recent years k-means has also been very popular algorithm to study in the Map-Reduce framework where the algorithm can easily be applied to cluster large amount of data sets in parallel \citep{Dean:2004}.

Guha et al. \citep{Guha:2003} designed an algorithm called STREAM that is based on the divide-and-conquer strategy and achieves a constant-factor approximation solving the k-median problem (a k-means variant). The algorithm divides the dataset into \textit{m} pieces of similar sizes. Each of the pieces are independently clustered sequentially and all the centers from all the pieces are then clustered further. They show a new k-median algorithm called LSEARCH that is used by the stream algorithm and is based on local search algorithm solving the facility location problem \citep{Charikar:1999} to solve the k-median problem. Results show that LSEARCH produced better quality clusters than k-means and the hierarchical algorithm BIRCH \citep{Zhang:1996} but took longer to run. In 2009 Ailon et al. \citep{Ailon:2009} extended the the work of Guha et al. introducing an one pass streaming algorithm for k-means with approximation guarantees. Achieving that they introduced a new algorithm called k-means\# that builds on the non-streaming algorithm k-means++ by Arthur and Vassilvitskii \citep{Arthur:2007} that is a combined algorithm of seeding the initial centers and running k-means. k-means\# provides a bi-criterion $ (\alpha,\beta) $ approximation algorithm by choosing $\alpha * k $ centers with approximation factor $\beta$. In the divide-and-conquer strategy they run the k-means\# independently on each piece of the data to achieve $O(k * log(k))$ random centers non-uniformly and use the k-means++ algorithm to find k centers from the intermediate centers from all the pieces of the data set. 

Another approach is using a coreset by selecting a weighted subset from the original data set such that by running any k-means algorithm on the subset will give near similar results to running k-means on the original data set. Ackermann et al. \citep{Ackermann:2010} introduce a new algorithm called StreamKM++ that uses k-means++ algorithm from Arthur et al. \citep{Arthur:2007} to solve k-means on the subset and the also design a new data structure called coreset tree to speed up the time for the sampling in the center initialization. StreamKM++ is a streaming version of k-means++ to cluster large data sets. Their approach was shown to be on par with LSEARCH algorithm in cluster quality but outperformed BIRCH by factor of 2. A recent work by Shindler et al. \citep{Shindler:2011} proposed an algorithm called \textit{Fast streaming k-means} based on the online facility location algorithm \citep{Meyerson:2001} and extends the work of Braverman et al. \citep{Braverman:2011} proving a faster running time and a better approximation factor. Their algorithm outperformed both work of Ailon et al. \citep{Ailon:2009} and Ackermann et al. \citep{Ackermann:2010} mentioned above.

Clustering data streams of an unknown length, evolving over time \citep{Aggarwal:2002, Aggarwal:2003Evolving} are challenging where it is not possible to access historic data points because of the amount of data arriving continuously. Aggarwal et al. \citep{Aggarwal:2003} proposed a well-known stream clustering framework called CluStream for clustering large evolving data streams and is guided by application-centered requirements. CluStream has an online component that maintains snapshots of statistical information about micro-clusters (a.k.a. \textit{cluster feature vector} \citep{Zhang:1996}) in a pyramidal time window and an offline component that uses the compact intermediate summary statistics from the micro-clusters to find higher level \textit{k} clusters using k-means, in a time horizon defined by an analyst. The authors proposed also a new high-dimensional, data stream clustering algorithm called HPStream that is highly scalable \citep{Aggarwal:2004}. HPStream uses projected clustering \citep{Aggarwal:1999}, which can determine clusters for a subset of dimensions, to data streams and a new data structure called \textit{fading cluster structure} that allows historical and current data to integrate nicely with a user-specified fading factor. Zhou et al. \citep{Zhou:2008} presented a clustering algorithm called SWClustering that clusters evolving data streams over sliding windows to be able to analyze also the evolution of the individual clusters by eliminating influence by historic data points while the new data points arrive. The Authors show that the CluStream algorithm is more sensitive to influences of outdated data and is less efficient.

Processing large of amount of data efficiently using parallel processing is an active research and gain much of popularity when the Google's MapReduce programming model was introduced by Dean and Ghemawat \citep{Dean:2004} that allows researchers easily to create highly scalable and fault tolerance algorithms by just implementing a Map and a Reduce function. Zhao et al. \citep{Zhao:2009} implemented a parallel version of k-means (PKMeans) in MapReduce framework and show that their algorithm can effectively run on large data sets. They designed the Map function to calculate the closest cluster centroid for each point at a time and output a $<key, value>$ pair where key is the closest cluster centroid id and value is the data point. After each map task they apply a combiner function called Combine that is executed on the same machine as the Map function. The Combine function partly sum the values of the data points assigned to the same cluster and outputs a $<key, value>$ pair where key is the id of the cluster centroid and the value is comprised of the sum values and the number of data points in that cluster. Using a combiner reduces the amount of intermediate information that is processed and sent over the network to the Reduce task \citep{Dean:2004, Zhao:2009}. Then the Reduce function sums up all the intermediate sub sum values from all the combiners and calculates the new centers for the next MapReduce iteration. To overcome the sensibility of k-means to outliers in data, Li et al. \citep{Li:2011} proposed an algorithm called MBK-means using the ensemble learning method called Bagging \citep{Breiman:1996}, generating \textit{k} new data sets from the original data set with replacement sampling. For each of the new data set the k-means is run using the MapReduce framework until convergence then finally the k sets of k centroids are merged to form the final k centroids.

Many extensions on the traditional MapReduce framework have been proposed to support algorithms running iteratively \citep{Condie:2010HadoopOnline, Ekanayake:2010Twister, Zaharia:2010Spark, Bu:2010HaLoop, Bu:2012HaLoop, Yan:2012IncMr} and incrementally \citep{Bhatotia:2011Incoop, Yan:2012IncMr, Bhatotia:2012Slider} efficiently. The incremental MapReduce frameworks are interesting and relates to our work since we are incrementally clustering data but the use of these frameworks are not in our study.

\section{Our study}
The algorithm in this thesis is most similar to PKMeans \citep{Zhao:2009} described above, a parallel k-means implementation in MapReduce using a Combine function to reduce the intermediate data sent between the mappers and the reducers. We however implement a parallel k-means algorithm in MapReduce so that each Map function efficiently calculates the distance to nearest cluster centers by calculating the distance matrix between all data points and the cluster centers instead of processing each data point separately. We apply the algorithm to incrementally but non-iteratively cluster player behaviors on theoretically large data sets that arrive daily. 

In our work we show k-means is a good approach to provide valuable insights into the general of behaviors for a specific real game data provided by GameAnalytics \citep{GA2013}. Work of Drachen et al. \citep{Drachen:2012, Drachen:2013} relates to ours when selecting and building important behavioral variables from user's telemetry and extracting behavioral profiles by analyzing and interpret the centroids (basis vectors) from the k-means algorithm running in MapReduce. Additionally we perform experiments with a controlled data set where we have different normal distributions of data arriving separately each day with results and future work are discussed.