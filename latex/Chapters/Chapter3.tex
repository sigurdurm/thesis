% Chapter 3

\chapter{Related Work} % Write in your own chapter title
\label{Chapter3}
\lhead{Chapter 3. \emph{Related Work}} % Write in your own chapter title to set the page header

In subsequent sections we will give a overview of some of the related and recent work. We start with looking into work related to Clustering Player Behaviors and then we dive into  work related to clustering large set of data using the k-means algorithm, where we can have a finite stream of data, an endless and evolving data stream and finally some parallel clustering implementations in the Map-Reduce framework. In our work we focus on processing a large amount of data of finite length in parallel using Map-Reduce and incrementally cluster data arriving each day. The behavior of the data can change from day to day like when dealing with an endless stream of data but is not in the scope, related work is presented and solutions are discussed in the Conclusions section. [TODO].

\section{Clustering Player Behaviors}
Many researches have been done on clustering and predicting player behaviors over the years to get a better understanding which kind of groups of behaviors are playing a game by data mining user telemetry that is being logged as a the game is being played.

\section{Clustering Large Data}
K-means is one of the most studied clustering algorithm out there and is still actively researched. It's a simple algorithm that partition the data into \textit{k} partitions by minimizing its objective function sum of squared error. From its appearance in a standard version by Stuart Lloyd in 1982 \citep{Lloyd:1982} , it has been one of the most popular clustering algorithm to research because of its simplicity. There are many different research areas regarding k-means e.g. manually set the number of \textit{k} partitions to cluster, initializing the centers of the partitions and dealing with outliers in data. In recent years k-means has also been very popular algorithm to study in the Map-Reduce framework where the k-means algorithm can easily be applied to cluster large amount of data sets in parallel \citep{Dean:2004}. Starting with work relating clustering stream of data of length \textit{n} that doesn't fit in memory, the incrementally evolving characteristics of endless data streams and recent work using the Map-Reduce framework.

We start with a work by Guha et al. \citep{Guha:2003} where they propose the STREAM algorithm that is based on the divide-and-conquer strategy and achieves a constant-factor approximation solving the k-median problem (a k-means variant). The algorithm divides the dataset into \textit{m} pieces of similar sizes. Each of the pieces are independently clustered sequentially and all the centers from all the pieces are then clustered further. They show a new k-median algorithm called LSEARCH that is used by the stream algorithm and is based on local search algorithm solving the facility location problem \citep{Charikar:1999} to solve the k-median problem. Results show that LSEARCH produced better quality clusters than k-means and the hierarchical algorithm BIRCH \citep{Zhang:1996} but took longer to run.

In 2009 Ailon et al. \citep{Ailon:2009} extended the the work of Guha et al. mentioned above and showed an one pass streaming algorithm for k-means with approximation guarantees. Achieving that they introduced a new algorithm called k-means\# that builds on the non-streaming algorithm k-means++ by Arthur et al. \citep{Arthur:2007} that is a combined algorithm of seeding the initial centers and running k-means. k-means\# provides a bi-criterion $ (\alpha,\beta) $ approximation algorithm by choosing $\alpha * k $ centers with approximation factor $\beta$. In the divide-and-conquer strategy they run the k-means\# independently on each piece of the data to achieve $O(k log k)$ random centers non-uniformly and use the k-means++ algorithm to find k centers from the intermediate centers from all the pieces of the data set. 

Another approach using a coreset by selecting a weighted subset from the original data set such that by running any k-means algorithm on the subset will give near similar results to running k-means on the original data set. Ackermann et al. \citep{Ackermann:2010} proposed a new algorithm called StreamKM++ that uses k-means++ algorithm from Arthur et al. \citep{Arthur:2007} to solve k-means on the subset and also proposed a new data structure called coreset tree to speed up the time for the sampling in the center initialization. StreamKM++ is a streaming version of k-means++ to cluster large data sets. Their approach was shown to be on par with LSEARCH algorithm in cluster quality but outperformed BIRCH by factor of 2. A recent work by Shindler et al. \citep{Shindler:2011} proposed an algorithm called \textit{Fast streaming k-means} based on the online facility location algorithm \citep{Meyerson:2001} and extends the work of Braverman et al. \citep{Braverman:2011} proving a faster running time and a better approximation factor. Their algorithm outperformed both work of Ailon et al. \citep{Ailon:2009} and Ackermann et al. \citep{Ackermann:2010} mentioned above.  

A different approach was introduced by Sculley in 2010 \citep{Sculley:2010}, modifications to k-means for batch optimizations. An algorithm called Mini-batch k-means that yields excellent clustering results with low computational cost for large datasets. The algorithm initializes the k centers like the normal k-means algorithm but then for each iteration it picks $b$ sized examples randomly from the dataset and for each point in the example the center which it is closest to is updated by taking a gradient descent step towards the point, with a learning rate of that center. The approach scales when datasets grow large with redundant examples and allowing convergence to better solutions compared to the on-line stochastic gradient descent (SGD) variant proposed by Bottou et al. \citep{Bottou:1995}.

Clustering a data stream where data arrives continuously and the behavior of the stream can change over time. This area of research is very popular in the recent years where generating data is rapid, has an unknown length and not possible to access historic data points seen before because of the amount of data.

Map-Reduce.
A parallel version of k-means++ initialization algorithm can be found in the work of Bahmani et al. from 2012 \citep{Bahmani:2012}. The algorithm is called k-means$||$ and is implemented in the Map-Reduce framework and they show it outperforms the k-means++ algorithm in both sequential and parallel settings.




\subsubsection{Map-Reduce}

\subsection{Data Streams}

